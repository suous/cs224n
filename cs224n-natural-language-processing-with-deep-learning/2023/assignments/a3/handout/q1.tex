\titledquestion{Machine Learning \& Neural Networks}[8] 
\begin{parts}

    
    \part[4] Adam Optimizer\newline
        Recall the standard Stochastic Gradient Descent update rule:
        \alns{
            	\btheta_{t+1} &\gets \btheta_t - \alpha \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)
        }
        where $t+1$ is the current timestep, $\btheta$ is a vector containing all of the model parameters, ($\btheta_t$ is the model parameter at time step $t$, and $\btheta_{t+1}$ is the model parameter at time step $t+1$), $J$ is the loss function, $\nabla_{\btheta} J_{\text{minibatch}}(\btheta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\alpha$ is the learning rate.
        Adam Optimization\footnote{Kingma and Ba, 2015, \url{https://arxiv.org/pdf/1412.6980.pdf}} uses a more sophisticated update rule with two additional steps.\footnote{The actual Adam update uses a few additional tricks that are less important, but we won't worry about them here. If you want to learn more about it, you can take a look at: \url{http://cs231n.github.io/neural-networks-3/\#sgd}}
            
        \begin{subparts}

            \subpart[2]First, Adam uses a trick called {\it momentum} by keeping track of $\bm$, a rolling average of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1}
                }
                where $\beta_1$ is a hyperparameter between 0 and 1 (often set to  0.9). Briefly explain in 2--4 sentences (you don't need to prove mathematically, just give an intuition) how using $\bm$ stops the updates from varying as much and why this low variance may be helpful to learning, overall.\newline

                \begin{enumerate}
                    \item The momentum term $\bm$ behaves as an expotiential moving average of the gradients, which reduces the variance of the parameter updates and helps to maintain a consistent direction towards the minimum.
                    \item SGD with momentum helps learning by smoothing out the updates to the model parameters. This smoothing effect helps the optimizer to avoid getting stuck in local minima and saddle points, and to converge faster to the global minimum of the loss landscape.
                \end{enumerate}
                
            \subpart[2] Adam extends the idea of {\it momentum} with the trick of {\it adaptive learning rates} by keeping track of  $\bv$, a rolling average of the magnitudes of the gradients:
                \alns{
                	\bm_{t+1} &\gets \beta_1\bm_{t} + (1 - \beta_1)\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \\
                	\bv_{t+1} &\gets \beta_2\bv_{t} + (1 - \beta_2) (\nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t) \odot \nabla_{\btheta_t} J_{\text{minibatch}}(\btheta_t)) \\
                	\btheta_{t+1} &\gets \btheta_t - \alpha \bm_{t+1} / \sqrt{\bv_{t+1}}
                }
                where $\odot$ and $/$ denote elementwise multiplication and division (so $\bz \odot \bz$ is elementwise squaring) and $\beta_2$ is a hyperparameter between 0 and 1 (often set to  0.99). Since Adam divides the update by $\sqrt{\bv}$, which of the model parameters will get larger updates?  Why might this help with learning?

                \begin{enumerate}
                    \item \textbf{Question 1:} The parameters with samller $\sqrt{v_{t+1}}$ will get larger updates. In other words, the parameters with smaller moving average of the squared gradients will get larger updates.
                    \item \textbf{Question 2:} Adam uses adaptive learning rates to adaptively adjust the learning rate for each parameter. This helps to speed up the learning process by making larger updates to the parameters with smaller gradients, and smaller updates to the parameters with larger gradients.
                    And because of the adaptive parameter updates, Adam usually converges faster than SGD with momentum, and is less sensitive to the choice of hyperparameters.
                \end{enumerate}
           
                
                \end{subparts}
        
        
            \part[4] 
            Dropout\footnote{Srivastava et al., 2014, \url{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}} is a regularization technique. During training, dropout randomly sets units in the hidden layer $\bh$ to zero with probability $p_{\text{drop}}$ (dropping different units each minibatch), and then multiplies $\bh$ by a constant $\gamma$. We can write this as:
                \alns{
                	\bh_{\text{drop}} = \gamma \bd \odot \bh
                }
                where $\bd \in \{0, 1\}^{D_h}$ ($D_h$ is the size of $\bh$)
                is a mask vector where each entry is 0 with probability $p_{\text{drop}}$ and 1 with probability $(1 - p_{\text{drop}})$. $\gamma$ is chosen such that the expected value of $\bh_{\text{drop}}$ is $\bh$:
                \alns{
                	\mathbb{E}_{p_{\text{drop}}}[\bh_\text{drop}]_i = h_i \text{\phantom{aaaa}}
                }
                for all $i \in \{1,\dots,D_h\}$. 
            \begin{subparts}
            \subpart[2]
                What must $\gamma$ equal in terms of $p_{\text{drop}}$? Briefly justify your answer or show your math derivation using the equations given above.

                \begin{align*} 
                    \mathbb{E}{p_{\text{drop}}}[\bh_\text{drop}] &= \mathbb{E}{p_{\text{drop}}}[\gamma \bd \odot \bh] \\ 
                    &= \gamma \mathbb{E}{p_{\text{drop}}}[\bd \odot \bh] \\
                    &= \gamma \mathbb{E}{p_{\text{drop}}}[\bd] \odot \bh \\
                    &= \gamma \times 1 \times (1-p_{\text{drop}}) \times \bh + \gamma \times 0 \times p_{\text{drop}} \times \bh \\
                    &= \gamma (1-p_{\text{drop}}) \bh
                \end{align*}

                Therefore, $\gamma = 1/(1-p_{\text{drop}})$.  This ensures that the expected value of the activations is the same during training and evaluation.

                
            
          \subpart[2] Why should dropout be applied during training? Why should dropout \textbf{NOT} be applied during evaluation? (Hint: it may help to look at the paper linked above in the write-up.) \newline

          \begin{enumerate}
            \item Dropout should be applied during training to prevent overfitting and improve the generalization performance of the model.
            \item By randomly dropping out units in each training iteration, dropout forces the remaining units to learn more robust and diverse features that are not dependent on the presence of any single unit.
            \item Dropout also acts as a form of model averaging and ensembling, since the model is trained on different subsets of the units in each iteration.
            \item Dropout should not be applied during evaluation because it would cause the model to make inconsistent and non-deterministic predictions for the same input.
          \end{enumerate}
         
        \end{subparts}


\end{parts}
