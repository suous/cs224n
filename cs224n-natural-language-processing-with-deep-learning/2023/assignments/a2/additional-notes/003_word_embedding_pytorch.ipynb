{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import seed_everything, read_cs224_sentences, pca\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b1573a2b8945a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff991cc-c2fa-4c52-a233-f6abd00b5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace(\n",
    "    data_path=Path(\"../code/utils/datasets/stanfordSentimentTreebank/\"),\n",
    "    num_context=3,\n",
    "    batch_size=64,\n",
    "    embedding_dim=10,\n",
    "    num_epochs=10,\n",
    "    lr=1e-2,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e9129-73d0-452b-a09a-6d05e04e0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveDataset(Dataset):\n",
    "    def __init__(self, path, unk_token=\"UNK\", num_context=2, flatten=False):\n",
    "        self._unk_token = unk_token\n",
    "        self._num_context = num_context\n",
    "        self.df, _, self._token_dict, self._token_freq = read_cs224_sentences(path, unk_token, nrows=5000)\n",
    "        sentences = self.df.sentence.apply(\n",
    "            lambda s: [unk_token] * num_context + s + [unk_token] * num_context\n",
    "        ).to_list()\n",
    "        context = np.concatenate(\n",
    "            [\n",
    "                np.lib.stride_tricks.sliding_window_view(self.sentence_to_index(s), num_context * 2 + 1)\n",
    "                for s in sentences\n",
    "            ]\n",
    "        )\n",
    "        center_word = context[:, num_context]\n",
    "        context = np.concatenate([context[:, :num_context], context[:, num_context + 1 :]], axis=1)\n",
    "        if flatten:\n",
    "            center_word = np.repeat(center_word, context.shape[1])\n",
    "            context = context.flatten()\n",
    "\n",
    "        self.data = list(zip(center_word, context))\n",
    "\n",
    "    def sentence_to_index(self, sentence):\n",
    "        return [self.word2idx.get(w, self.word2idx[self._unk_token]) for w in sentence]\n",
    "\n",
    "    @property\n",
    "    def idx2word(self):\n",
    "        return list(self._token_dict.keys())\n",
    "\n",
    "    @property\n",
    "    def word2idx(self):\n",
    "        return self._token_dict\n",
    "\n",
    "    @property\n",
    "    def word_freq(self):\n",
    "        return self._token_freq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class SkipGramMode(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self._embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self._linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._embedding(x)\n",
    "        x = self._linear(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        return self._embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return model, train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def valid(valid_loader, model, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            valid_loss += loss.item()\n",
    "    return valid_loss / len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6bdd3-aec4-45ec-900f-443e7d0ec756",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dataset = NaiveDataset(path=config.data_path / \"datasetSentences.txt\", num_context=3)\n",
    "tmp_loader = DataLoader(tmp_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"The total number of unique tokens is {len(tmp_dataset.word_freq)}\")\n",
    "print(f\"The total number of dataset is {len(tmp_dataset)}\")\n",
    "\n",
    "display(tmp_dataset.df.head())\n",
    "print(f\"The top 10 most common words are:\")\n",
    "display(tmp_dataset.word_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Call the data generator to get one batch and its targets\n",
    "inputs, targets = next(iter(tmp_loader))\n",
    "\n",
    "print(f\"The inputs shape is {inputs.shape}\")\n",
    "print(f\"The targets shape is {targets.shape}\")\n",
    "\n",
    "print(f\"The center and context words are:\")\n",
    "for c, o in zip(inputs, targets):\n",
    "    print(tmp_dataset.idx2word[c], [tmp_dataset.idx2word[w] for w in o])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "933b6f7a11232438"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = NaiveDataset(path=config.data_path / \"datasetSentences.txt\", num_context=config.num_context, flatten=True)\n",
    "data_loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "inputs, targets = next(iter(data_loader))\n",
    "\n",
    "model = SkipGramMode(vocab_size=len(dataset.word2idx), embedding_dim=config.embedding_dim)\n",
    "output = model(inputs)\n",
    "print(f\"The inputs shape is {inputs.shape}\")\n",
    "print(f\"The targets shape is {targets.shape}\")\n",
    "print(f\"The output shape is {output.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67208b4ecf0a6d6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_digits = int(math.log10(config.num_epochs)) + 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "moving_loss = None\n",
    "\n",
    "losses = defaultdict(list)\n",
    "for epoch in range(config.num_epochs + 1):\n",
    "    model, loss = train(train_loader=data_loader, model=model, criterion=criterion, optimizer=optimizer)\n",
    "    if moving_loss is None:\n",
    "        moving_loss = loss\n",
    "    else:\n",
    "        moving_loss = 0.95 * moving_loss + 0.05 * loss\n",
    "    losses[\"loss\"].append(loss)\n",
    "    losses[\"moving_loss\"].append(moving_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:0{num_digits}d}/{config.num_epochs} Loss: {moving_loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603f5a7939ee99f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=len(losses.keys()), figsize=(10, 10), sharex=True)\n",
    "\n",
    "for ax, (k, v) in zip(axs, losses.items()):\n",
    "    ax.plot(v)\n",
    "    ax.set_title(k)\n",
    "    ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee968be4a080535"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"great\",\n",
    "    \"cool\",\n",
    "    \"brilliant\",\n",
    "    \"wonderful\",\n",
    "    \"well\",\n",
    "    \"amazing\",\n",
    "    \"worth\",\n",
    "    \"sweet\",\n",
    "    \"enjoyable\",\n",
    "    \"boring\",\n",
    "    \"bad\",\n",
    "    \"dumb\",\n",
    "    \"annoying\",\n",
    "    \"female\",\n",
    "    \"male\",\n",
    "    \"queen\",\n",
    "    \"king\",\n",
    "    \"man\",\n",
    "    \"woman\",\n",
    "    \"rain\",\n",
    "    \"snow\",\n",
    "    \"hail\",\n",
    "    \"coffee\",\n",
    "    \"tea\",\n",
    "]\n",
    "\n",
    "words = [w for w in words if w in dataset.word2idx]\n",
    "\n",
    "word_indices = [dataset.word2idx[w] for w in words]\n",
    "result = pca(model.embeddings[word_indices], 2)\n",
    "result = result / np.linalg.norm(result, axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ea34b56cfca1ccc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
