{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from utils import read_corpus, Vocab, collate_fn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size=1024, hidden_size=768, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim=embed_size)\n",
    "        self.post_embed_cnn = nn.Conv1d(embed_size, embed_size, kernel_size=2, padding=\"same\")\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.h_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src: Tensor, source_lengths: Tensor) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        x = self.embedding(src)  # (batch_size, src_len, embed_size)\n",
    "        x = self.post_embed_cnn(x.permute(0, 2, 1))  # (batch_size, embed_size, src_len)\n",
    "        x = pack_padded_sequence(\n",
    "            x.permute(0, 2, 1), source_lengths, batch_first=True, enforce_sorted=False\n",
    "        )  # (batch_size, src_len, embed_size)\n",
    "        enc_hidden, (last_hidden, last_cell) = self.encoder(x)\n",
    "        # (batch_size, src_len, 2*hidden_size), (batch_size, 2, hidden_size), (batch_size, 2, hidden_size)\n",
    "        enc_hidden, _ = pad_packed_sequence(enc_hidden, batch_first=True)  # (batch_size, src_len, hidden_size*2)\n",
    "        init_decoder_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        init_decoder_hidden = self.h_projection(init_decoder_hidden)  # (batch_size, hidden_size)\n",
    "        init_decoder_cell = torch.cat((last_cell[0], last_cell[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        init_decoder_cell = self.c_projection(init_decoder_cell)  # (batch_size, hidden_size)\n",
    "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
    "        return enc_hidden, dec_init_state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_size, embed_size=1024, hidden_size=768, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(target_size, embedding_dim=embed_size)\n",
    "        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n",
    "        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.target_vocab_projection = nn.Linear(hidden_size, target_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        enc_hidden: torch.Tensor,\n",
    "        enc_masks: torch.Tensor,\n",
    "        dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "        target_padded: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Chop off the <END> token for max length sentences.\n",
    "        target_padded = target_padded[:, :-1]\n",
    "        # Initialize the decoder state (hidden and cell)\n",
    "        dec_state = dec_init_state\n",
    "\n",
    "        # Initialize previous combined output vector o_{t-1} as zero\n",
    "        batch_size = enc_hidden.size(0)\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=enc_hidden.device)  # (batch_size, hidden_size)\n",
    "\n",
    "        # Initialize a list we will use to collect the combined output o_t on each step\n",
    "        combined_outputs = []\n",
    "        enc_hidden_proj = self.att_projection(enc_hidden)  # (batch_size, src_len, hidden_size)\n",
    "        y = self.embedding(target_padded)  # (batch_size, tgt_len, embed_size)\n",
    "        for y_t in torch.split(y, split_size_or_sections=1, dim=1):\n",
    "            y_t = y_t.squeeze(dim=1)  # (batch_size, embed_size)\n",
    "            ybar_t = torch.cat((y_t, o_prev), dim=1)  # (batch_size, embed_size+hidden_size)\n",
    "            o_t, dec_state = self.step(\n",
    "                ybar_t=ybar_t,\n",
    "                dec_state=dec_state,\n",
    "                enc_hidden=enc_hidden,\n",
    "                enc_hidden_proj=enc_hidden_proj,\n",
    "                enc_masks=enc_masks,\n",
    "            )\n",
    "            combined_outputs.append(o_t)\n",
    "            o_prev = o_t\n",
    "        combined_outputs = torch.stack(combined_outputs)  # (tgt_len, batch_size, hidden_size)\n",
    "        return self.target_vocab_projection(combined_outputs.permute(1, 0, 2))  # (batch_size, tgt_len, vocab_size)\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        ybar_t: torch.Tensor,\n",
    "        dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "        enc_hidden: torch.Tensor,\n",
    "        enc_hidden_proj: torch.Tensor,\n",
    "        enc_masks: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        dec_state = self.decoder(ybar_t, dec_state)\n",
    "        dec_hidden, _ = dec_state  # (batch_size, hidden_size)\n",
    "        # e_t = torch.bmm(enc_hidden_proj, dec_hidden.unsqueeze(dim=2)).squeeze(dim=2)  # (batch_size, src_len)\n",
    "        e_t = torch.einsum(\"bsh,bh->bs\", enc_hidden_proj, dec_hidden)  # (batch_size, src_len)\n",
    "\n",
    "        # Set e_t to -inf where enc_masks has 1\n",
    "        if enc_masks is not None:\n",
    "            e_t.data.masked_fill_(enc_masks.bool(), -float(\"inf\"))\n",
    "\n",
    "        alpha_t = torch.softmax(e_t, dim=1)  # (batch_size, src_len)\n",
    "        # a_t = torch.bmm(alpha_t.unsqueeze(dim=1), enc_hidden).squeeze(dim=1)  # (batch_size, 2*hidden_size)\n",
    "        a_t = torch.einsum(\"bs,bsh->bh\", alpha_t, enc_hidden)  # (batch_size, 2*hidden_size)\n",
    "        u_t = torch.cat((dec_hidden, a_t), dim=1)  # (batch_size, 3*hidden_size)\n",
    "        v_t = self.combined_output_projection(u_t)  # (batch_size, hidden_size)\n",
    "        o_t = self.dropout(F.tanh(v_t))  # (batch_size, hidden_size)\n",
    "        return o_t, dec_state\n",
    "\n",
    "\n",
    "def generate_sent_masks(enc_hidden: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
    "    # enc_masks = torch.zeros( enc_hidden.size()[:2], dtype=torch.float, device=enc_hidden.device)\n",
    "    # for i, l in enumerate(source_lengths):\n",
    "    #     enc_masks[i, l:] = 1\n",
    "    # return enc_masks\n",
    "    return torch.where(torch.arange(enc_hidden.size(1)) < torch.tensor(source_lengths).unsqueeze(1), 0, 1)\n",
    "\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, source_lengths: List[int]) -> Tensor:\n",
    "        encoder_hidden, dec_init_state = self.encoder(src=src, source_lengths=source_lengths)\n",
    "        enc_masks = generate_sent_masks(encoder_hidden, source_lengths)\n",
    "        decoder_hidden = self.decoder(\n",
    "            enc_hidden=encoder_hidden, enc_masks=enc_masks, dec_init_state=dec_init_state, target_padded=tgt\n",
    "        )\n",
    "        target_masks = (tgt != 0).float()\n",
    "        probs = F.log_softmax(decoder_hidden, dim=-1)\n",
    "        target_gold_words_log_prob = (\n",
    "            torch.gather(probs, index=tgt[:, 1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[:, 1:]\n",
    "        )\n",
    "        return target_gold_words_log_prob.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "target_size = 200\n",
    "batch_size = 32\n",
    "src_len = 10\n",
    "src = torch.randint(0, input_size, (batch_size, src_len))\n",
    "tgt = torch.randint(0, target_size, (batch_size, src_len))\n",
    "source_lengths = torch.randint(1, src_len, (batch_size,)).tolist()\n",
    "\n",
    "encoder = Encoder(input_size)\n",
    "decoder = Decoder(target_size)\n",
    "nmt = NMT(encoder, decoder)\n",
    "scores = nmt(src, tgt, source_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "shuffle = True\n",
    "\n",
    "data_path = Path(\"../code\")\n",
    "\n",
    "train_data_src = read_corpus(data_path.joinpath(\"zh_en_data\", \"train_debug.zh\"), data_path / \"src.model\")\n",
    "train_data_tgt = read_corpus(data_path.joinpath(\"zh_en_data\", \"train_debug.en\"), data_path / \"tgt.model\")\n",
    "\n",
    "valid_data_src = read_corpus(data_path.joinpath(\"zh_en_data\", \"dev.en\"), data_path / \"src.model\")\n",
    "valid_data_tgt = read_corpus(data_path.joinpath(\"zh_en_data\", \"dev.zh\"), data_path / \"tgt.model\")\n",
    "\n",
    "vocab = Vocab.load(data_path / \"vocab.json\")\n",
    "print(vocab)\n",
    "\n",
    "train_data_indices_src = vocab.src.words2indices(train_data_src)\n",
    "train_data_indices_tgt = vocab.tgt.words2indices(train_data_tgt)\n",
    "\n",
    "valid_data_indices_src = vocab.src.words2indices(valid_data_src)\n",
    "valid_data_indices_tgt = vocab.tgt.words2indices(valid_data_tgt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = list(zip(train_data_indices_src, train_data_indices_tgt))\n",
    "valid_data = list(zip(valid_data_indices_src, valid_data_indices_tgt))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "src_sentences, tgt_sentences, _ = next(iter(train_loader))\n",
    "print(f\"src shape: {src_sentences.shape}, tgt shape: {tgt_sentences.shape}\")\n",
    "\n",
    "print(\"src and tgt sentences:\")\n",
    "for src_sent, tgt_sent in zip(src_sentences, tgt_sentences):\n",
    "    src_sent = \"\".join(vocab.src.indices2words(src_sent.tolist()))\n",
    "    tgt_sent = \"\".join(vocab.tgt.indices2words(tgt_sent.tolist()))\n",
    "    print(\"-\" * 100)\n",
    "    print(src_sent.replace(\"<pad>\", \"\").replace(\"▁\", \" \"))\n",
    "    print(tgt_sent.replace(\"<pad>\", \"\").replace(\"▁\", \" \").replace(\"</s>\", \"\").replace(\"<s>\", \"\"))\n",
    "    print(\"-\" * 100)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, optimizer):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor, input_lengths = data\n",
    "        optimizer.zero_grad()\n",
    "        loss = -model(input_tensor, target_tensor, input_lengths).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train(train_dataloader, model, n_epochs, learning_rate=0.001):\n",
    "    num_digits = int(math.log10(n_epochs)) + 1\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    moving_loss = None\n",
    "    losses = defaultdict(list)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, model, optimizer)\n",
    "        if moving_loss is None:\n",
    "            moving_loss = loss\n",
    "        else:\n",
    "            moving_loss = 0.95 * moving_loss + 0.05 * loss\n",
    "        losses[\"loss\"].append(loss)\n",
    "        losses[\"moving_loss\"].append(moving_loss)\n",
    "        print(f\"Epoch {epoch+1:0{num_digits}d}/{n_epochs} Loss: {moving_loss:.4f}\")\n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_size = len(vocab.src)\n",
    "target_size = len(vocab.tgt)\n",
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size=hidden_size).to(device)\n",
    "decoder = Decoder(target_size, hidden_size=hidden_size).to(device)\n",
    "nmt = NMT(encoder, decoder).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "losses = train(train_loader, nmt, 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=len(losses.keys()), figsize=(10, 10), sharex=True)\n",
    "\n",
    "for ax, (k, v) in zip(axs, losses.items()):\n",
    "    ax.plot(v)\n",
    "    ax.set_title(k)\n",
    "    ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
