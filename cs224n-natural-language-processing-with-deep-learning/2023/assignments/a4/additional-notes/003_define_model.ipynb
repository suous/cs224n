{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size=1024, hidden_size=768, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim=embed_size)\n",
    "        self.post_embed_cnn = nn.Conv1d(embed_size, embed_size, kernel_size=2, padding=\"same\")\n",
    "        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n",
    "        self.h_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.c_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src: Tensor, source_lengths: Tensor) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        x = self.embedding(src)  # (batch_size, src_len, embed_size)\n",
    "        x = self.post_embed_cnn(x.permute(0, 2, 1))  # (batch_size, embed_size, src_len)\n",
    "        x = pack_padded_sequence(\n",
    "            x.permute(0, 2, 1), source_lengths, batch_first=True, enforce_sorted=False\n",
    "        )  # (batch_size, src_len, embed_size)\n",
    "        enc_hidden, (last_hidden, last_cell) = self.encoder(x)\n",
    "        # (batch_size, src_len, 2*hidden_size), (batch_size, 2, hidden_size), (batch_size, 2, hidden_size)\n",
    "        enc_hidden, _ = pad_packed_sequence(enc_hidden, batch_first=True)  # (batch_size, src_len, hidden_size*2)\n",
    "        init_decoder_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        init_decoder_hidden = self.h_projection(init_decoder_hidden)  # (batch_size, hidden_size)\n",
    "        init_decoder_cell = torch.cat((last_cell[0], last_cell[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        init_decoder_cell = self.c_projection(init_decoder_cell)  # (batch_size, hidden_size)\n",
    "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
    "        return enc_hidden, dec_init_state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_size, embed_size=1024, hidden_size=768, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(target_size, embedding_dim=embed_size)\n",
    "        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n",
    "        self.att_projection = nn.Linear(2 * hidden_size, hidden_size, bias=False)\n",
    "        self.combined_output_projection = nn.Linear(3 * hidden_size, hidden_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.target_vocab_projection = nn.Linear(hidden_size, target_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        enc_hidden: torch.Tensor,\n",
    "        enc_masks: torch.Tensor,\n",
    "        dec_init_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "        target_padded: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        # Chop off the <END> token for max length sentences.\n",
    "        target_padded = target_padded[:, :-1]\n",
    "        # Initialize the decoder state (hidden and cell)\n",
    "        dec_state = dec_init_state\n",
    "\n",
    "        # Initialize previous combined output vector o_{t-1} as zero\n",
    "        batch_size = enc_hidden.size(0)\n",
    "        o_prev = torch.zeros(batch_size, self.hidden_size, device=enc_hidden.device)  # (batch_size, hidden_size)\n",
    "\n",
    "        # Initialize a list we will use to collect the combined output o_t on each step\n",
    "        combined_outputs = []\n",
    "        enc_hidden_proj = self.att_projection(enc_hidden)  # (batch_size, src_len, hidden_size)\n",
    "        y = self.embedding(target_padded)  # (batch_size, tgt_len, embed_size)\n",
    "        for y_t in torch.split(y, split_size_or_sections=1, dim=1):\n",
    "            y_t = y_t.squeeze(dim=1)  # (batch_size, embed_size)\n",
    "            ybar_t = torch.cat((y_t, o_prev), dim=1)  # (batch_size, embed_size+hidden_size)\n",
    "            o_t, dec_state = self.step(\n",
    "                ybar_t=ybar_t,\n",
    "                dec_state=dec_state,\n",
    "                enc_hidden=enc_hidden,\n",
    "                enc_hidden_proj=enc_hidden_proj,\n",
    "                enc_masks=enc_masks,\n",
    "            )\n",
    "            combined_outputs.append(o_t)\n",
    "            o_prev = o_t\n",
    "        combined_outputs = torch.stack(combined_outputs)  # (tgt_len, batch_size, hidden_size)\n",
    "        return self.target_vocab_projection(combined_outputs.permute(1, 0, 2))  # (batch_size, tgt_len, vocab_size)\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        ybar_t: torch.Tensor,\n",
    "        dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
    "        enc_hidden: torch.Tensor,\n",
    "        enc_hidden_proj: torch.Tensor,\n",
    "        enc_masks: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        dec_state = self.decoder(ybar_t, dec_state)\n",
    "        dec_hidden, _ = dec_state  # (batch_size, hidden_size)\n",
    "        # e_t = torch.bmm(enc_hidden_proj, dec_hidden.unsqueeze(dim=2)).squeeze(dim=2)  # (batch_size, src_len)\n",
    "        e_t = torch.einsum(\"bsh,bh->bs\", enc_hidden_proj, dec_hidden)  # (batch_size, src_len)\n",
    "\n",
    "        # Set e_t to -inf where enc_masks has 1\n",
    "        if enc_masks is not None:\n",
    "            e_t.data.masked_fill_(enc_masks.bool(), -float(\"inf\"))\n",
    "\n",
    "        alpha_t = torch.softmax(e_t, dim=1)  # (batch_size, src_len)\n",
    "        # a_t = torch.bmm(alpha_t.unsqueeze(dim=1), enc_hidden).squeeze(dim=1)  # (batch_size, 2*hidden_size)\n",
    "        a_t = torch.einsum(\"bs,bsh->bh\", alpha_t, enc_hidden)  # (batch_size, 2*hidden_size)\n",
    "        u_t = torch.cat((dec_hidden, a_t), dim=1)  # (batch_size, 3*hidden_size)\n",
    "        v_t = self.combined_output_projection(u_t)  # (batch_size, hidden_size)\n",
    "        o_t = self.dropout(F.tanh(v_t))  # (batch_size, hidden_size)\n",
    "        return o_t, dec_state\n",
    "\n",
    "\n",
    "def generate_sent_masks(enc_hidden: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
    "    # enc_masks = torch.zeros( enc_hidden.size()[:2], dtype=torch.float, device=enc_hidden.device)\n",
    "    # for i, l in enumerate(source_lengths):\n",
    "    #     enc_masks[i, l:] = 1\n",
    "    # return enc_masks\n",
    "    return torch.where(torch.arange(enc_hidden.size(1)) < torch.tensor(source_lengths).unsqueeze(1), 0, 1)\n",
    "\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, source_lengths: List[int]) -> Tensor:\n",
    "        encoder_hidden, dec_init_state = self.encoder(src=src, source_lengths=source_lengths)\n",
    "        enc_masks = generate_sent_masks(encoder_hidden, source_lengths)\n",
    "        decoder_hidden = self.decoder(\n",
    "            enc_hidden=encoder_hidden, enc_masks=enc_masks, dec_init_state=dec_init_state, target_padded=tgt\n",
    "        )\n",
    "        target_masks = (tgt != 0).float()\n",
    "        probs = F.log_softmax(decoder_hidden, dim=-1)\n",
    "        target_gold_words_log_prob = (\n",
    "            torch.gather(probs, index=tgt[:, 1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[:, 1:]\n",
    "        )\n",
    "        return target_gold_words_log_prob.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "target_size = 200\n",
    "batch_size = 32\n",
    "src_len = 10\n",
    "src = torch.randint(0, input_size, (batch_size, src_len))\n",
    "tgt = torch.randint(0, target_size, (batch_size, src_len))\n",
    "source_lengths = torch.randint(1, src_len, (batch_size,)).tolist()\n",
    "\n",
    "encoder = Encoder(input_size)\n",
    "decoder = Decoder(target_size)\n",
    "nmt = NMT(encoder, decoder)\n",
    "scores = nmt(src, tgt, source_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
